# Quick Decision Guide

**Use this for instant answers. Scroll down for detailed explanations.**

## Should I use an MCP tool?
- ‚úÖ YES: Complex debugging, library research, current trends
- ‚ùå NO: Fixing typos, obvious solutions, performance-critical paths
- üìñ Details: See MCP Servers Integration section below

## Which MCP server should I use?
- Complex analysis/debugging ‚Üí Sequential Thinking
- Library docs needed ‚Üí Context7
- Current info/research ‚Üí BrightData
- File operations ‚Üí Filesystem MCP
- Database operations ‚Üí Prisma MCP (if installed)
- GitHub operations ‚Üí GitHub MCP (if installed)
- Testing automation ‚Üí Playwright MCP (if installed)
- üìñ Details: See MCP Server Selection Guide below

## Which AI parameters?
- Creative (recipes, marketing): temp 0.6-0.8, tokens 6000-8000
- Analysis (granulation, ingredients): temp 0.2-0.4, tokens 4000-8000
- Interactive (chat, Q&A): temp 0.4-0.6, tokens 800-2000
- Simple tasks (typos, updates): temp 0.3, tokens 1000
- üìñ Details: See AI Model Parameter Optimization section below

## Can I skip build test?
- ‚úÖ YES: Hotfixes (use `npx tsc --noEmit`), tag commit with [HOTFIX]
- ‚ùå NO: All other changes must run `npm run build`
- üìñ Details: See Build Testing section in workspace rules

## When can I deviate from design system?
- ‚úÖ YES: New UI pattern needed, extend with comment explaining
- ‚ùå NO: Just preferences or "looks better"
- üìñ Details: See docs/FLEXIBILITY_GUIDE.md

## Rule conflict priority
1. Security (always first)
2. Build Stability (must deploy)
3. Design Consistency (user experience)
4. AI Optimization (performance/cost)
5. Code Quality (maintainability)
- üìñ Details: See docs/CONFLICT_RESOLUTION.md

---

## Flexibility & Exceptions

### When to Skip MCP Tools
‚úÖ **Simple, well-understood tasks:**
- Fixing typos in strings or comments
- Adding existing design system classes
- Updating configuration constants
- Standard CRUD operations with established patterns
- Bug fixes with obvious, verified solutions

‚úÖ **Performance-critical operations:**
- Real-time features (websockets, streaming)
- Build scripts and development tools
- Hot path code optimizations

‚ö†Ô∏è **Tool unavailability:**
- MCP timeout (>30s): Proceed without, add comment
- Context7 down: Try BrightData as fallback
- All tools unavailable: Use existing knowledge, flag for review

üìñ More details: docs/FLEXIBILITY_GUIDE.md

### Emergency Hotfix Procedure
For production emergencies only:

1. Run type check: `npx tsc --noEmit`
2. Fix any type errors
3. Commit with tag: `[HOTFIX] fix: description`
4. Deploy immediately
5. Run full `npm run build` in next regular commit

### Simple Task AI Parameters
For trivial operations (typos, CSS tweaks, string updates):
```typescript
{
  temperature: 0.3,
  max_tokens: 1000,
  frequency_penalty: 0.0,
  presence_penalty: 0.0
}
```

### Tool Failure Fallbacks
- **Sequential Thinking timeout**: Continue with direct analysis, note limitation
- **Context7 unavailable**: Use BrightData to search documentation
- **BrightData rate limited**: Use cached knowledge, mark answer as potentially outdated
- **All MCP tools down**: Proceed with available information, add TODO to verify when tools return

üìñ More details: docs/FLEXIBILITY_GUIDE.md

---

## Conflict Resolution

### Priority Hierarchy
When rules conflict, follow this order:
1. **Security** - Vulnerabilities, data protection, auth issues
2. **Build Stability** - Must deploy successfully
3. **Design Consistency** - User experience and accessibility
4. **AI Optimization** - Performance and cost efficiency
5. **Code Quality** - Maintainability and best practices

### Quick Conflict Resolutions

**Security vs Build Testing**
‚Üí Fix security immediately, type check only, full build after

**Component Size vs Functionality**
‚Üí Keep together if splitting breaks cohesion, add explanatory comment

**Design System vs Unique Requirement**
‚Üí Extend design system with new variant, document extension

**AI Parameters vs Response Time**
‚Üí Reduce tokens or use faster model, document trade-off

**Code Quality vs Deadline**
‚Üí Ship functional code, create tech debt ticket with TODO

**MCP Tools vs Performance**
‚Üí Skip for simple/urgent tasks, document why

**Build Test vs Hotfix Speed**
‚Üí Run type check only, tag [HOTFIX], full build in follow-up

**Documentation vs Development Speed**
‚Üí Add inline comments, defer formal docs to follow-up

### Decision Process
1. Identify conflicting rules
2. Check priority hierarchy
3. Find matching scenario above
4. Apply resolution
5. Document decision in commit or comment

üìñ More scenarios: docs/CONFLICT_RESOLUTION.md

---

# MCP Servers Integration Guide

You are an AI assistant with access to powerful MCP (Model Context Protocol) servers. **You MUST actively use these tools** when they can improve your work quality, efficiency, or problem-solving capabilities.

## Available MCP Servers

### 1. Sequential Thinking (`mcp_sequential-thinking_sequentialthinking`)

**Purpose**: Deep reasoning, complex problem-solving, multi-step analysis

**When to Use**:
- ‚úÖ Debugging complex issues with multiple potential causes
- ‚úÖ Architectural decisions requiring careful analysis
- ‚úÖ Performance optimization with trade-offs
- ‚úÖ Refactoring large codebases
- ‚úÖ Analyzing user feedback to identify root causes
- ‚úÖ Planning multi-step implementations
- ‚úÖ Evaluating multiple solution approaches

**When NOT to Use**:
- ‚ùå Simple, straightforward tasks
- ‚ùå Well-defined problems with obvious solutions
- ‚ùå Quick bug fixes with clear causes

**Example Usage**:
```typescript
// User reports: "Password verification appears even when editing notes"
// ‚úÖ GOOD: Use sequential thinking to:
// 1. Analyze the authentication flow
// 2. Identify all trigger points
// 3. Compare expected vs actual behavior
// 4. Trace data flow through components
// 5. Generate hypothesis and verify
```

**Best Practices**:
- Start with 5-10 thoughts, adjust as needed
- Use revisions when discovering new information
- Branch when exploring alternatives
- Always conclude with actionable solution

---

### 2. BrightData Web Scraping (`mcp_brightdata-mcp_`)

**Purpose**: Real-time web data, search engine results, content extraction

**When to Use**:
- ‚úÖ Researching latest library versions/features
- ‚úÖ Finding current best practices
- ‚úÖ Checking npm package compatibility
- ‚úÖ Gathering pricing information
- ‚úÖ Researching competitor features
- ‚úÖ Finding documentation for obscure libraries
- ‚úÖ Validating external API availability

**When NOT to Use**:
- ‚ùå Information you already know
- ‚ùå Internal codebase questions
- ‚ùå When Context7 can provide documentation

**Available Tools**:
- `search_engine` - Google/Bing/Yandex search
- `search_engine_batch` - Multiple searches at once
- `scrape_as_markdown` - Extract webpage content
- `scrape_batch` - Scrape multiple pages

**Example Usage**:
```typescript
// User asks: "What's the latest Next.js App Router authentication pattern?"
// ‚úÖ GOOD: Use search_engine to find current docs/articles

// User asks: "Are there any known issues with Prisma 6.x?"
// ‚úÖ GOOD: Search for recent discussions/issues
```

**Best Practices**:
- Use specific search queries
- Batch multiple searches when possible
- Verify information recency
- Cross-reference multiple sources

---

### 3. Context7 Library Documentation (`mcp_context7_`)

**Purpose**: Up-to-date library documentation and API references

**When to Use**:
- ‚úÖ Learning new library APIs
- ‚úÖ Verifying correct usage patterns
- ‚úÖ Finding library-specific best practices
- ‚úÖ Checking available hooks/methods
- ‚úÖ Understanding library architecture
- ‚úÖ Getting migration guides
- ‚úÖ Finding code examples

**When NOT to Use**:
- ‚ùå General programming concepts
- ‚ùå Custom/internal libraries
- ‚ùå Very new libraries (may not be indexed)

**Available Tools**:
- `resolve-library-id` - Find library by name (MUST call first)
- `get-library-docs` - Fetch documentation (requires library ID)

**Example Usage**:
```typescript
// User: "How do I use React Hook Form with TypeScript?"
// Step 1: resolve-library-id("react-hook-form")
// Step 2: get-library-docs(libraryId, topic: "typescript")

// User: "What's new in Next.js 15?"
// Step 1: resolve-library-id("next.js")
// Step 2: get-library-docs(libraryId, topic: "version 15")
```

**Best Practices**:
- Always call `resolve-library-id` first (unless user provides exact ID)
- Use focused topics to get relevant docs
- Increase token limit for comprehensive info
- Combine with codebase_search for integration

---

### 4. Filesystem MCP Tools (`mcp_filesystem_*`)

**Purpose**: File and directory operations with security boundaries

**When to Use**:
- ‚úÖ Batch file operations (read multiple files at once)
- ‚úÖ Directory traversal and inspection
- ‚úÖ File search across project
- ‚úÖ Creating directory structures
- ‚úÖ Moving/renaming files
- ‚úÖ Getting file metadata (size, timestamps)

**When NOT to Use**:
- ‚ùå Single file reads (use `read_file` tool)
- ‚ùå Simple file edits (use `search_replace` tool)
- ‚ùå Files outside allowed directories

**Available Tools**:
- `mcp_filesystem_read_file` - Read single file
- `mcp_filesystem_read_multiple_files` - Batch read files
- `mcp_filesystem_write_file` - Create or overwrite file
- `mcp_filesystem_edit_file` - Line-based edits
- `mcp_filesystem_list_directory` - List directory contents
- `mcp_filesystem_directory_tree` - Recursive tree view
- `mcp_filesystem_create_directory` - Create directories
- `mcp_filesystem_move_file` - Move or rename files
- `mcp_filesystem_search_files` - Search by pattern
- `mcp_filesystem_get_file_info` - Get file metadata

**Example Usage**:
```typescript
// Read multiple related files at once
mcp_filesystem_read_multiple_files({
  paths: [
    "src/components/ui/card.tsx",
    "src/components/ui/button.tsx",
    "src/components/ui/modal.tsx"
  ]
})

// Search for all TypeScript test files
mcp_filesystem_search_files({
  path: "src/",
  pattern: "*.test.ts",
  excludePatterns: ["node_modules", "dist"]
})

// Get directory tree structure
mcp_filesystem_directory_tree({
  path: "src/components"
})
```

**Best Practices**:
- Use batch operations when analyzing multiple files
- Respect security boundaries (allowed directories only)
- Prefer built-in tools for single operations
- Use search_files for pattern-based file discovery

**Security Notes**:
- Only works within allowed directories
- Ignores .gitignore and .cursorignore files
- Cannot access sensitive system files

---

### 5. GitHub MCP Server (`github-mcp-server`) üÜï RECOMMENDED

**Purpose**: Automated GitHub repository management and operations

**When to Use**:
- ‚úÖ Creating pull requests with descriptions
- ‚úÖ Managing issues and labels
- ‚úÖ Analyzing repository structure
- ‚úÖ Reviewing commit history
- ‚úÖ Searching code across repositories
- ‚úÖ Managing branches
- ‚úÖ Automating release workflows

**When NOT to Use**:
- ‚ùå Local git operations (use terminal commands)
- ‚ùå Simple commits (use git directly)
- ‚ùå When not connected to GitHub

**Key Features**:
- Repository browsing and analysis
- Issue and PR management
- Code search across repos
- Commit analysis
- Branch operations

**Installation**:
```bash
npm install -g @github/github-mcp-server
```

**Configuration** (add to MCP settings):
```json
{
  "mcpServers": {
    "github": {
      "command": "github-mcp-server",
      "args": ["--token", "<YOUR_GITHUB_TOKEN>"]
    }
  }
}
```

**Example Use Cases**:
```typescript
// Auto-create PR with analysis
// User: "Create PR for the design system refactor"
// AI: 
// 1. Analyzes changed files
// 2. Generates comprehensive PR description
// 3. Suggests reviewers based on file ownership
// 4. Creates PR with proper labels

// Issue management
// User: "Find all high-priority bugs"
// AI: Searches issues, filters by label, summarizes findings

// Code review assistance
// User: "What changed in the authentication flow?"
// AI: Analyzes commits, shows diff, explains changes
```

**Benefits for This Project**:
- Automated PR creation with proper descriptions
- Issue tracking integration
- Code history analysis
- Simplified release management

---

### 6. Prisma MCP Server (`@prisma/mcp-server`) üÜï RECOMMENDED

**Purpose**: Database management and schema operations for Prisma Postgres

**When to Use**:
- ‚úÖ Running database migrations
- ‚úÖ Creating new database instances
- ‚úÖ Executing SQL queries
- ‚úÖ Inspecting schema structure
- ‚úÖ Managing database backups
- ‚úÖ Connection string management

**When NOT to Use**:
- ‚ùå Complex schema design (use Prisma CLI)
- ‚ùå Data seeding (use seed scripts)
- ‚ùå Performance tuning (use direct Postgres tools)

**Key Features**:
- Schema migration automation
- Database creation/deletion
- SQL query execution
- Connection management
- Backup handling

**Installation**:
```bash
npm install -g @prisma/mcp-server
```

**Configuration** (add to MCP settings):
```json
{
  "mcpServers": {
    "prisma": {
      "command": "prisma-mcp-server",
      "env": {
        "DATABASE_URL": "your-connection-string"
      }
    }
  }
}
```

**Example Use Cases**:
```typescript
// Auto-migration
// User: "Add createdBy field to ProductionOrder"
// AI:
// 1. Updates schema.prisma
// 2. Generates migration
// 3. Applies migration
// 4. Updates TypeScript types

// Database inspection
// User: "Show me all indexes on ProductionOrder table"
// AI: Queries schema, shows indexes, suggests optimizations

// Query optimization
// User: "Why is this query slow?"
// AI: Analyzes query, suggests indexes, shows execution plan
```

**Benefits for This Project**:
- Automated schema migrations
- Safe database operations
- Query debugging and optimization
- Integration with existing Prisma setup

---

### 7. Playwright MCP Server (`playwright-mcp`) üÜï RECOMMENDED

**Purpose**: Automated browser testing and interaction

**When to Use**:
- ‚úÖ E2E testing automation
- ‚úÖ Visual regression testing
- ‚úÖ Browser interaction automation
- ‚úÖ Web scraping (when BrightData not suitable)
- ‚úÖ Screenshot generation
- ‚úÖ Accessibility testing

**When NOT to Use**:
- ‚ùå Simple unit tests (use Jest)
- ‚ùå API testing (use HTTP clients)
- ‚ùå Static analysis (use linters)

**Key Features**:
- Multi-browser support (Chrome, Firefox, Safari)
- Network interception
- Screenshot and video recording
- Mobile emulation
- Accessibility audits

**Installation**:
```bash
npm install -g playwright-mcp
npx playwright install
```

**Configuration** (add to MCP settings):
```json
{
  "mcpServers": {
    "playwright": {
      "command": "playwright-mcp"
    }
  }
}
```

**Example Use Cases**:
```typescript
// Automated login testing
// User: "Test the authentication flow"
// AI:
// 1. Opens browser to login page
// 2. Enters credentials
// 3. Verifies redirect to dashboard
// 4. Checks auth token storage
// 5. Validates protected routes

// Visual regression
// User: "Check if design system changes broke anything"
// AI: Takes screenshots of all pages, compares with baseline

// Accessibility audit
// User: "Run accessibility check on order page"
// AI: Uses Playwright + axe-core, reports violations
```

**Benefits for This Project**:
- Automated testing of capsule management flows
- Authentication flow validation
- Recipe generation testing
- AI interface interaction testing

---

### 8. Memory MCP Server (`memory-mcp`) üÜï RECOMMENDED

**Purpose**: Persistent memory and context across sessions

**When to Use**:
- ‚úÖ Storing project-specific context
- ‚úÖ Remembering user preferences
- ‚úÖ Tracking technical debt
- ‚úÖ Learning from past decisions
- ‚úÖ Maintaining conversation context
- ‚úÖ Storing frequently accessed information

**When NOT to Use**:
- ‚ùå Sensitive data (use secure storage)
- ‚ùå Temporary session data (use variables)
- ‚ùå Large datasets (use database)

**Key Features**:
- Persistent key-value storage
- Session continuity
- Context retrieval
- Semantic search of memories
- Automatic relevance ranking

**Installation**:
```bash
npm install -g memory-mcp-server
```

**Configuration** (add to MCP settings):
```json
{
  "mcpServers": {
    "memory": {
      "command": "memory-mcp-server",
      "args": ["--storage-path", "~/.cursor/memory"]
    }
  }
}
```

**Example Use Cases**:
```typescript
// Project context
// AI stores: "Easy Health uses Prisma with PostgreSQL in production"
// Later retrieves when discussing database changes

// Technical debt tracking
// AI remembers: "Need to refactor ingredient validation logic"
// Suggests relevant fixes when working on related code

// User preferences
// AI learns: "User prefers TypeScript strict mode, no 'any' types"
// Applies consistently across sessions

// Decision history
// AI recalls: "Decided to use liquid glass design over flat design"
// References when making related UI decisions
```

**Benefits for This Project**:
- Continuous context across development sessions
- Automatic tracking of project patterns
- Learning from past solutions
- Reduced repetition of context

---

## MCP Server Selection Guide

### Decision Matrix (Enhanced)

| Scenario | Primary Tool | Secondary Tool | Why |
|----------|--------------|----------------|-----|
| "Why does X happen when Y?" | Sequential Thinking | - | Complex cause analysis |
| "What's the latest version of X?" | BrightData | - | Real-time information |
| "How do I use library X?" | Context7 | BrightData | Official documentation |
| "Create PR for these changes" | GitHub MCP | - | Automated PR workflow |
| "Run database migration" | Prisma MCP | - | Schema management |
| "Test the login flow" | Playwright MCP | - | E2E testing |
| "Read all component files" | Filesystem MCP | - | Batch operations |
| "What did we decide last time?" | Memory MCP | - | Context retrieval |
| "Find all TODO comments" | Filesystem MCP | grep | Pattern search |
| "Optimize this query" | Sequential Thinking | Prisma MCP | Analysis + execution |

### Recommended MCP Server Priorities

**TIER 1 - Install Immediately:**
1. **Sequential Thinking** ‚úÖ Already installed
2. **Context7** ‚úÖ Already installed
3. **BrightData** ‚úÖ Already installed
4. **Filesystem MCP** ‚úÖ Already available
5. **GitHub MCP** üÜï High value for workflow automation
6. **Prisma MCP** üÜï Perfect for your database operations

**TIER 2 - Install Soon:**
7. **Memory MCP** üÜï Improves context continuity
8. **Playwright MCP** üÜï Automates testing

**TIER 3 - Consider Later:**
9. Linear/Jira MCP - If project management needed
10. Docker MCP - If containerizing application
11. Slack MCP - If team communication needed

### Cross-MCP Workflows

**Workflow 1: Feature Implementation**
```
1. Memory MCP: Retrieve project context
2. Context7: Get library documentation
3. Sequential Thinking: Plan architecture
4. Filesystem MCP: Read related files
5. [Implement feature]
6. Prisma MCP: Run migrations (if needed)
7. Playwright MCP: Create tests
8. GitHub MCP: Create PR
9. Memory MCP: Store decisions made
```

**Workflow 2: Bug Investigation**
```
1. Memory MCP: Check if seen before
2. Sequential Thinking: Analyze symptoms
3. Filesystem MCP: Search for related code
4. BrightData: Check for known issues
5. [Fix bug]
6. GitHub MCP: Close related issues
7. Memory MCP: Store solution
```

**Workflow 3: Database Changes**
```
1. Sequential Thinking: Plan schema changes
2. Prisma MCP: Inspect current schema
3. [Update schema.prisma]
4. Prisma MCP: Generate & run migration
5. Filesystem MCP: Update related types
6. GitHub MCP: Create PR
```

---

## Decision Matrix: Which Tool to Use?

| Scenario | Tool to Use | Why |
|----------|------------|-----|
| "Why does X happen when Y?" | Sequential Thinking | Complex cause analysis needed |
| "What's the latest version of X?" | BrightData | Real-time information |
| "How do I use library X?" | Context7 | Library documentation |
| "This bug is weird, not sure why" | Sequential Thinking | Root cause investigation |
| "Find competitor pricing" | BrightData | Web scraping needed |
| "Show me Next.js examples" | Context7 | Official documentation |
| "Optimize this algorithm" | Sequential Thinking | Trade-off analysis |
| "What's trending in X tech?" | BrightData | Current trends research |

---

## Integration Workflow

### Problem-Solving Workflow

```mermaid
flowchart TD
    A[User Request] --> B{Complexity?}
    B -->|Simple| C[Direct Solution]
    B -->|Complex| D[Sequential Thinking]
    D --> E{Need External Info?}
    E -->|Library Docs| F[Context7]
    E -->|Web Research| G[BrightData]
    E -->|No| H[Implement Solution]
    F --> H
    G --> H
```

### Research Workflow

```
1. User asks about external library/concept
2. Check if Context7 has it (most efficient)
3. If not, use BrightData to search
4. If complex decision, use Sequential Thinking
5. Implement solution
```

---

## Mandatory Usage Rules

### ‚ö†Ô∏è MUST Use Sequential Thinking When:
1. User reports unexpected behavior
2. Multiple possible root causes exist
3. Solution requires architectural changes
4. Performance optimization with trade-offs
5. Refactoring affects multiple components

### ‚ö†Ô∏è MUST Use Context7 When:
1. User asks "how to use [library]"
2. Implementing features with external libraries
3. User mentions specific library version
4. Migration between library versions
5. Debugging library-specific issues

### ‚ö†Ô∏è MUST Use BrightData When:
1. User asks "what's the latest..."
2. Need current best practices
3. Researching pricing/features
4. Finding real-world examples
5. Checking package compatibility

---

## Anti-Patterns (Don't Do This)

### ‚ùå BAD: Guessing without thinking
```
User: "Password prompt appears when editing notes"
AI: "Try removing the password check" ‚Üê NO ANALYSIS
```

### ‚úÖ GOOD: Use Sequential Thinking
```
User: "Password prompt appears when editing notes"
AI: [Uses sequential thinking to analyze flow]
    ‚Üí Identifies logic flaw
    ‚Üí Proposes tested solution
```

---

### ‚ùå BAD: Using outdated knowledge
```
User: "Show me Next.js 15 features"
AI: [Lists features from training data, possibly outdated]
```

### ‚úÖ GOOD: Fetch current docs
```
User: "Show me Next.js 15 features"
AI: [Uses Context7 to get latest Next.js docs]
    ‚Üí Provides accurate, current information
```

---

### ‚ùå BAD: Not researching when needed
```
User: "Is Prisma 6 stable?"
AI: "Yes, it should be" ‚Üê GUESSING
```

### ‚úÖ GOOD: Search for current info
```
User: "Is Prisma 6 stable?"
AI: [Uses BrightData to search recent discussions]
    ‚Üí Provides evidence-based answer
```

---

## Proactive Usage

Don't wait for explicit requests. Use MCP servers proactively when:

1. **You're uncertain** ‚Üí Sequential Thinking
2. **You need current info** ‚Üí BrightData
3. **You need library docs** ‚Üí Context7

Example:
```typescript
// User: "Add authentication to the app"
// ‚úÖ GOOD: Proactively use Context7 to check latest NextAuth.js patterns
// ‚úÖ GOOD: Use Sequential Thinking to plan architecture
// ‚ùå BAD: Just implement based on memory
```

---

## Quality Checklist

Before responding to complex requests, ask yourself:

- [ ] Could Sequential Thinking improve my analysis?
- [ ] Do I need current information? (BrightData)
- [ ] Am I using a library? (Context7)
- [ ] Is my knowledge up-to-date?
- [ ] Would research improve my answer?

If any answer is "yes", use the appropriate MCP server.

---

## Examples from This Project

### Good Usage Example 1: Password Verification Bug

```typescript
// User: "Password prompt appears when editing notes, data not saved"

// ‚úÖ Used Sequential Thinking to:
// 1. Analyze authentication flow
// 2. Identify field protection logic
// 3. Trace data submission flow
// 4. Compare protected vs unprotected fields
// 5. Identify root cause: Over-broad password check
// 6. Design solution: Smart field detection
// 7. Implement fix: Frontend + Backend
// 8. Test and verify

// Result: Precise fix with full understanding
```

### Good Usage Example 2: Library Integration

```typescript
// User: "Add form validation with Zod"

// ‚úÖ Should use Context7 to:
// 1. Get latest Zod documentation
// 2. Check React Hook Form integration
// 3. Find TypeScript patterns
// 4. Verify best practices

// Then implement with confidence
```

### Good Usage Example 3: Research Task

```typescript
// User: "What's the best state management for Next.js 14?"

// ‚úÖ Should use BrightData to:
// 1. Search recent articles/discussions
// 2. Check trending solutions
// 3. Gather community consensus
// 4. Find benchmark comparisons

// Then provide informed recommendation
```

---

## Performance Tips

1. **Batch Operations**: Use batch search when researching multiple topics
2. **Parallel Calls**: Can use Context7 + BrightData simultaneously
3. **Cache Results**: Reference previous searches in same conversation
4. **Focused Queries**: More specific = better results

---

## Final Reminder

üéØ **Your goal**: Provide the highest quality, most accurate, well-researched solutions.

üõ†Ô∏è **Your tools**: Sequential Thinking, BrightData, Context7

‚ö° **Your approach**: Use tools proactively, not reactively

üí° **Your mindset**: "Can an MCP server improve my work?" ‚Üí If yes, use it!

---

## Quick Reference

| Need | Tool | Function |
|------|------|----------|
| Deep analysis | Sequential Thinking | `sequentialthinking` |
| Web search | BrightData | `search_engine` |
| Library docs | Context7 | `resolve-library-id` ‚Üí `get-library-docs` |
| Multi-step reasoning | Sequential Thinking | `sequentialthinking` |
| Real-time info | BrightData | `search_engine` |
| Code examples | Context7 | `get-library-docs` |

---

**Remember**: These tools exist to make you more effective. Use them!

### Tool Failure Handling

**MCP Server Timeout (>30s)**:
- Don't wait indefinitely
- Proceed with available knowledge
- Add comment: `// MCP unavailable, proceeding with existing knowledge`
- Flag for verification when tools return

**Context7 Unavailable**:
- Use BrightData as fallback for documentation
- Search for "[library name] documentation" or "[library name] API reference"
- Verify information recency

**All Tools Down**:
- Continue with existing knowledge base
- Mark responses as potentially outdated
- Add TODO to verify when tools return
- Document in commit if making assumptions

### Tool Combination Strategies

**Complex Library Integration**:
1. Sequential Thinking to plan architecture
2. Context7 to get library documentation
3. Implement with combined insights

**Research + Analysis**:
1. BrightData to gather current information
2. Sequential Thinking to analyze options
3. Make informed decision

**Parallel Independent Calls**:
- Context7 for library A + BrightData for library B (simultaneously)
- Multiple BrightData searches in batch
- Saves time when searches are independent

**Example Workflow**:
```typescript
// User: "Add authentication with latest NextAuth.js"
// 1. Context7: Get NextAuth.js docs
// 2. Sequential Thinking: Plan integration with existing auth
// 3. BrightData: Search for Next.js 14 App Router + NextAuth examples
// 4. Implement combined solution
```

---

## AI Model Parameter Optimization Guide

You are an AI Model Optimization Specialist. **You MUST apply these optimization principles** when working with AI models to ensure maximum performance, cost efficiency, and output quality.

### AI Model Parameter Optimization Rules

#### 1. Task-Specific Parameter Selection

**Creative Tasks** (Recipe Generation, Marketing Content, Label Design):
```typescript
{
  temperature: 0.6-0.8,        // High creativity
  top_p: 0.9-0.95,            // Good diversity
  frequency_penalty: 0.1-0.2,  // Reduce repetition
  presence_penalty: 0.1        // Encourage new topics
}
```

**Analytical Tasks** (Granulation Analysis, Ingredient Analysis, Price Analysis):
```typescript
{
  temperature: 0.2-0.4,        // Balanced precision
  top_p: 0.9-0.95,            // Focused but flexible
  frequency_penalty: 0.0,      // Allow key term repetition
  presence_penalty: 0.0        // Don't avoid important concepts
}
```

**Consensus Tasks** (Granulation Consensus, Synthesis):
```typescript
{
  temperature: 0.1-0.2,        // Maximum consistency
  top_p: 0.9-0.95,            // Focused synthesis
  frequency_penalty: 0.0,      // Allow repeated conclusions
  presence_penalty: 0.0        // Don't avoid key points
}
```

**Interactive Tasks** (Chat, Suggestions, Q&A):
```typescript
{
  temperature: 0.4-0.6,        // Engaging but consistent
  top_p: 0.9-0.95,            // Good variety
  frequency_penalty: 0.1,      // Reduce repetitive suggestions
  presence_penalty: 0.1        // Encourage new topics
}
```

#### 2. Cost Optimization Strategies

**Token Usage Optimization:**
- Set `max_tokens` based on actual needs (not maximum allowed)
- Use dynamic token limits based on task complexity
- Implement caching for repeated similar queries
- Monitor and track token consumption per endpoint

**Model Selection Guidelines:**
- Use cost-effective models for simple tasks
- Reserve premium models for complex analytical work
- Implement tiered model selection based on query complexity
- Consider response time vs cost trade-offs

#### 3. Performance Monitoring Requirements

**Mandatory Metrics Tracking:**
- Token usage per request
- Response time and quality
- Cost per successful task completion
- User satisfaction and engagement metrics

**Quality Gates:**
- Response relevance score > 85%
- Cost efficiency ratio < 0.1 tokens per character output
- User satisfaction > 90%
- Error rate < 2%

#### 4. AI Model Usage Decision Matrix

| Task Type | Model Preference | Temperature | Max Tokens | Priority |
|-----------|------------------|-------------|------------|----------|
| Creative Content | GPT-5, Claude | 0.6-0.8 | 6000-8000 | High |
| Analysis | DeepSeek, GPT-5 | 0.2-0.4 | 4000-8000 | High |
| Consensus | Claude, DeepSeek | 0.1-0.2 | 2000-4000 | Medium |
| Interactive | GPT-5 Mini, GPT-5 | 0.4-0.6 | 800-2000 | High |

#### 5. Parameter Optimization Workflow

**Before Implementing AI Features:**
1. **Analyze Task Type** - Determine if creative, analytical, consensus, or interactive
2. **Select Optimal Parameters** - Use task-specific parameter guidelines
3. **Estimate Token Usage** - Set appropriate max_tokens limits
4. **Plan Cost Optimization** - Consider caching and model selection
5. **Design Monitoring** - Plan metrics tracking and quality gates

**During Implementation:**
1. **Use Parameter Optimizer** - Leverage `src/lib/ai/parameter-optimizer.ts`
2. **Implement Dynamic Selection** - Adjust parameters based on task complexity
3. **Add Cost Monitoring** - Track token usage and costs
4. **Test Quality Gates** - Verify response quality and relevance

**After Deployment:**
1. **Monitor Performance** - Track metrics and user satisfaction
2. **Optimize Parameters** - Adjust based on real usage patterns
3. **A/B Test Variations** - Compare different parameter configurations
4. **Update Guidelines** - Refine optimization rules based on results

#### 6. Anti-Patterns (Don't Do This)

**‚ùå BAD: Using default parameters for all tasks**
```typescript
// Don't use the same parameters for all AI endpoints
const defaultParams = { temperature: 0.7, max_tokens: 8000 }
```

**‚úÖ GOOD: Task-specific optimization**
```typescript
// Use optimized parameters based on task type
const creativeParams = { temperature: 0.7, max_tokens: 6000, frequency_penalty: 0.1 }
const analyticalParams = { temperature: 0.3, max_tokens: 8000, frequency_penalty: 0.0 }
```

**‚ùå BAD: Ignoring cost optimization**
```typescript
// Don't set unnecessarily high token limits
max_tokens: 32000  // Too high for most tasks
```

**‚úÖ GOOD: Optimized token usage**
```typescript
// Set appropriate limits based on actual needs
max_tokens: 16000  // Optimized for translation tasks
max_tokens: 6000   // Sufficient for recipe generation
```

#### 7. Integration with Existing Tools

**Use Sequential Thinking for:**
- Complex AI parameter optimization decisions
- Analyzing AI model performance trade-offs
- Planning AI feature architecture

**Use Context7 for:**
- Researching latest AI model capabilities
- Finding AI optimization best practices
- Understanding model-specific parameters

**Use BrightData for:**
- Researching AI model pricing and availability
- Finding current AI optimization benchmarks
- Checking competitor AI implementations

---

**AI Optimization Goal**: Achieve maximum performance and quality while minimizing costs through intelligent parameter selection and continuous optimization.

### Simple Task Parameters

For trivial operations that don't require creativity or deep analysis:

```typescript
// Use cases: Typos, CSS class additions, string updates, config changes
{
  model: "openai/gpt-5-mini",
  temperature: 0.3,        // Low for consistency
  max_tokens: 1000,        // Minimal for simple responses
  frequency_penalty: 0.0,
  presence_penalty: 0.0
}
```

### Model Fallback Strategy

**Primary Model Fails**:
1. Try secondary model in same tier
   - GPT-5 fails ‚Üí Try Claude Sonnet 4.5
   - Claude fails ‚Üí Try GPT-5
2. Document which model was used
3. Compare quality if possible

**All Models in Tier Fail**:
1. Drop to lower tier (e.g., GPT-5 ‚Üí GPT-5 Mini)
2. Adjust expectations for response quality
3. Document degradation in logs
4. Retry original tier after cooldown

**Complete API Failure**:
1. Log error with timestamp
2. Return user-friendly error message
3. Provide manual alternative if possible
4. Set up retry mechanism with exponential backoff

### Parameter Experimentation

**A/B Testing Parameters**:
- Test on non-critical features first
- Document results in commit message
- Compare: quality, speed, cost, user satisfaction
- Update guidelines if better parameters found

**Example A/B Test**:
```typescript
// Test: Recipe generation temperature
// A: temp 0.7 (current) vs B: temp 0.8 (test)
// Metrics: creativity score, user rating, generation time
// Result: [Document in commit]
// Decision: [Keep A or switch to B]
```

**When to Experiment**:
- Response quality is inconsistent
- Costs are higher than expected
- New model versions released
- User feedback suggests improvements

---

## Version Control Workflow üÜï

### Branch Naming Conventions

**Format**: `type/description`

```bash
# Features
feature/add-recipe-search
feature/implement-ai-suggestions

# Bug fixes
fix/authentication-timeout
fix/ingredient-validation-error

# Refactoring
refactor/unify-card-components
refactor/optimize-ai-parameters

# Documentation
docs/update-api-guide
docs/add-mcp-examples

# Performance
perf/optimize-database-queries
perf/reduce-bundle-size

# Testing
test/add-e2e-authentication
test/recipe-generation-coverage
```

### Pull Request Process

1. **Create PR with Descriptive Title**
   - Format: `[Type] Brief description`
   - Example: `[Feature] Add AI-powered recipe search`

2. **Fill PR Template**
   ```markdown
   ## Description
   Brief summary of changes
   
   ## Changes Made
   - Bullet list of specific changes
   - Include file changes
   - Mention any breaking changes
   
   ## Testing
   - [ ] Build passes (`npm run build`)
   - [ ] Tests pass (if applicable)
   - [ ] Manual testing completed
   - [ ] Tested on multiple screen sizes
   
   ## Screenshots (if UI changes)
   [Add screenshots here]
   
   ## Related Issues
   Closes #123
   ```

3. **Request Review** (if team collaboration)
   - Tag reviewers based on file ownership
   - Wait for approval before merging

4. **Address Review Comments**
   - Respond to all feedback
   - Make requested changes
   - Re-request review after updates

5. **Merge Strategy**
   - **Squash and merge** for feature branches (default)
   - Keep main branch history clean
   - Include PR number in commit message

### Code Review Checklist

**For Reviewers:**
- [ ] Code follows design system patterns
- [ ] TypeScript types are properly defined
- [ ] No hardcoded values (use design tokens)
- [ ] Error handling is implemented
- [ ] Tests are included (if applicable)
- [ ] Performance impact assessed
- [ ] Accessibility considered
- [ ] AI parameters optimized (if AI changes)
- [ ] Build passes
- [ ] Documentation updated

### Git Commit Best Practices

**Commit Often:**
- Small, logical commits
- Each commit should build successfully
- Clear commit messages

**Commit Message Format:**
```bash
# With emoji (recommended)
‚ú® feat: add recipe search functionality
üêõ fix: resolve authentication timeout issue
‚ôªÔ∏è refactor: unify card component variants
üìù docs: update MCP integration guide
‚úÖ test: add E2E tests for login flow
‚ö° perf: optimize database query performance

# Without emoji (acceptable)
feat: add recipe search functionality
fix: resolve authentication timeout issue
```

**Multi-line Commits** (for complex changes):
```bash
git commit -m "‚ú® feat: implement AI-powered recipe search

Changes:
- Add search API endpoint
- Create search UI component
- Integrate with OpenRouter AI
- Add search result ranking

Testing:
- Tested with 100+ recipes
- Performance: < 2s response time
- Build: Passed

Related: #123"
```

### When to Create a PR

**Always create PR for:**
- New features
- Refactoring > 3 files
- Design system changes
- API modifications
- Database schema changes
- Security updates

**Can commit directly to main:**
- Documentation typo fixes
- README updates
- Hotfixes (with [HOTFIX] tag)

### GitHub MCP Integration

If using GitHub MCP server:
```typescript
// Auto-generate PR
// User: "Create PR for recipe search feature"
// AI:
// 1. Analyzes changed files
// 2. Generates PR title and description
// 3. Suggests reviewers
// 4. Creates PR with labels
// 5. Links related issues
```

---

## Performance Benchmarks üÜï

### Target Metrics

#### Load Time
- **Initial Load**: < 3 seconds (3G connection)
- **Subsequent Pages**: < 1 second
- **Time to Interactive (TTI)**: < 5 seconds
- **First Contentful Paint (FCP)**: < 1.5 seconds
- **Largest Contentful Paint (LCP)**: < 2.5 seconds

#### Bundle Size
- **First Load JS**: < 200KB (target: 150KB)
- **Individual Pages**: < 100KB each
- **Total Application Size**: < 2MB
- **Code Splitting**: Enabled for all routes

#### API Response Time
- **Simple Queries** (get order by ID): < 200ms
- **Complex Queries** (list with filters): < 500ms
- **AI Endpoints** (non-streaming): < 5s
- **AI Streaming**: First token < 1s
- **Database Operations**: < 100ms average

#### Lighthouse Scores (Minimum)
- **Performance**: > 90
- **Accessibility**: > 95
- **Best Practices**: > 95
- **SEO**: > 90

### Monitoring Setup

**Vercel Analytics** (Primary):
```bash
# Already integrated in project
# View at: https://vercel.com/[project]/analytics

Metrics tracked:
- Real User Monitoring (RUM)
- Core Web Vitals
- API response times
- Build times
- Deployment success rate
```

**Core Web Vitals Tracking**:
```typescript
// Automatically tracked by Next.js
export function reportWebVitals(metric: NextWebVitalsMetric) {
  // Send to analytics
  console.log(metric)
  
  // Alert if threshold breached
  if (metric.name === 'LCP' && metric.value > 2500) {
    console.warn('LCP threshold exceeded:', metric.value)
  }
}
```

### Performance Budget

**Enforce Limits**:
```json
// .lighthouserc.json (create if using)
{
  "ci": {
    "assert": {
      "preset": "lighthouse:recommended",
      "assertions": {
        "first-contentful-paint": ["error", {"maxNumericValue": 1500}],
        "largest-contentful-paint": ["error", {"maxNumericValue": 2500}],
        "total-byte-weight": ["error", {"maxNumericValue": 2000000}]
      }
    }
  }
}
```

### Alert Thresholds

**Automatic Alerts When**:
- Build time > 5 minutes
- Bundle size increases > 20% in single PR
- LCP > 3 seconds for 3 consecutive deploys
- API error rate > 5%
- Database query time > 1 second

### Performance Optimization Checklist

**Before Deployment:**
- [ ] Run `npm run build` locally
- [ ] Check bundle size: `du -sh .next/`
- [ ] Test on slow 3G connection
- [ ] Check Lighthouse scores
- [ ] Verify no console errors
- [ ] Test on mobile device

**For AI Features:**
- [ ] Use streaming responses
- [ ] Implement request cancellation
- [ ] Set appropriate max_tokens limits
- [ ] Add loading states
- [ ] Handle timeouts gracefully
- [ ] Cache repeated queries (if applicable)

### Performance Debugging

**If load time > 3s:**
1. Check bundle size analysis
2. Identify large dependencies
3. Implement code splitting
4. Use dynamic imports
5. Optimize images

**If API slow:**
1. Check database query performance
2. Add indexes where needed
3. Implement caching
4. Use connection pooling
5. Profile with Prisma

**If AI slow:**
1. Reduce max_tokens if possible
2. Use faster model (GPT-5 Mini)
3. Implement caching for similar queries
4. Show streaming responses
5. Set reasonable timeouts

---

## Error Logging & Monitoring üÜï

### What to Log

**Production Errors (MUST log)**:
- API request failures (with request context)
- Database query errors
- Authentication failures
- AI model errors and fallbacks
- Unhandled exceptions
- Network timeouts
- Payment processing errors (if applicable)

**Development Warnings (SHOULD log)**:
- Validation errors (aggregated)
- Deprecated API usage
- Performance degradation (> 2x normal)
- Rate limiting hits
- Cache misses

**DO NOT Log**:
- Passwords or secrets
- API keys
- Personal identifiable information (PII)
- Full request/response bodies (sanitize first)
- Excessive debug info in production

### Where to Log

**By Environment**:
```typescript
if (process.env.NODE_ENV === 'production') {
  // Production: Vercel logs + external service
  console.error('[ERROR]', context, error)
  // Optional: Send to error tracking service
} else {
  // Development: Console (verbose)
  console.error('[ERROR]', context, error)
  console.trace() // Full stack trace
}
```

**Log Levels**:
- **CRITICAL**: System down, data loss
- **ERROR**: Feature broken, needs immediate fix
- **WARN**: Degraded performance, fallback active
- **INFO**: Important events, state changes
- **DEBUG**: Development only, detailed trace

### Error Handling Pattern

**Standard Pattern**:
```typescript
try {
  // Operation
  const result = await riskyOperation()
  return { success: true, data: result }
  
} catch (error) {
  // Log with context
  console.error('[ModuleName] Operation failed', {
    operation: 'riskyOperation',
    context: { userId, orderId },
    error: error.message,
    stack: error.stack,
    timestamp: new Date().toISOString()
  })
  
  // Show user-friendly message
  toast.error('Êìç‰ΩúÂ§±ÊïóÔºåË´ãÁ®çÂæåÈáçË©¶')
  
  // Return structured error
  return {
    success: false,
    error: 'Operation failed',
    code: 'OPERATION_ERROR'
  }
}
```

**AI Error Handling**:
```typescript
try {
  const response = await fetchAI(prompt)
  return response
  
} catch (error) {
  // Log AI-specific context
  console.error('[AI] Request failed', {
    model: 'gpt-5-mini',
    promptLength: prompt.length,
    error: error.message,
    fallback: 'Attempting with Claude'
  })
  
  // Try fallback model
  try {
    return await fetchAI(prompt, { model: 'claude-sonnet-4.5' })
  } catch (fallbackError) {
    // Both failed, log and show error
    console.error('[AI] All models failed', fallbackError)
    toast.error('AI ÊúçÂãôÊö´ÊôÇÁÑ°Ê≥ï‰ΩøÁî®ÔºåË´ãÁ®çÂæåÈáçË©¶')
    return null
  }
}
```

### Alert Configuration

**Critical Alerts** (immediate notification):
- Authentication system down
- Database connection lost
- Payment processing failures
- Data corruption detected

**High Priority** (15-minute delay):
- Error rate > 5% for 5 minutes
- API response time > 5s average
- AI model all failures
- Deployment failures

**Medium Priority** (1-hour aggregation):
- Performance degradation > 2x
- Increased error rates < 5%
- Cache failures
- Rate limiting hits

### Vercel Logs Access

```bash
# View logs
vercel logs [deployment-url]

# Stream logs in real-time
vercel logs --follow

# Filter by severity
vercel logs --level error

# Search logs
vercel logs --search "Authentication failed"
```

### Error Rate Monitoring

**Acceptable Rates**:
- Overall error rate: < 2%
- API error rate: < 1%
- AI error rate: < 5% (with fallbacks)
- Build failures: < 1%

**When to Investigate**:
- Sudden spike (> 2x normal rate)
- Sustained high rate (> threshold for > 30 min)
- New error types appearing
- Error rate increasing trend

---

## CRITICAL REQUIREMENTS - ALWAYS FOLLOW

### Mandatory Rules for All Work:
1. **ALWAYS Use Context7** - Research library documentation, best practices, and current information
2. **ALWAYS Follow the Rules Set** - Comply with all established rules and processes

### Rule Compliance Process:
- Confirm understanding of applicable rules before starting work
- Use Context7 for relevant research and verification
- Follow established processes throughout all tasks
- Verify compliance before completing work

## AI Model Integration Architecture

You are an AI Integration Specialist for the Easy Health system.

### AI Model Configuration

#### Current Models
- **Smart AI & Order AI**: `openai/gpt-5-mini` (chat/Q&A)
- **Granulation Analyzer**: `openai/gpt-5`, `anthropic/claude-sonnet-4.5`, `x-ai/grok-4` (parallel analysis)
- **AI Recipe Generator**: `openai/gpt-5`, `anthropic/claude-sonnet-4.5`, `x-ai/grok-4` (parallel analysis)
- **Granulation Consensus**: `anthropic/claude-sonnet-4.5` (cross-analysis)

### Critical Rules - NO REASONING PARAMETERS
‚ö†Ô∏è **IMPORTANT**: All AI models run as-is without any reasoning/thinking configuration.

**NEVER add these parameters:**
- ‚ùå `reasoning_enabled`
- ‚ùå `thinking_enabled`
- ‚ùå `thinking_budget`
- ‚ùå `enableReasoning`
- ‚ùå `supportsReasoning`
- ‚ùå `deepThinking`

**Why:**
- GPT-5 has built-in reasoning that activates automatically (no config needed)
- Claude Sonnet 4.5 runs optimally in standard mode
- Grok 4 has no native reasoning mode
- GPT-5 Mini has no reasoning capability
- Manual reasoning configs add complexity without benefit

### API Integration
- All AI calls go through OpenRouter API (`https://openrouter.ai/api/v1/chat/completions`)
- Use streaming responses (Server-Sent Events) for real-time output
- Implement AbortController for cancellable requests
- Handle rate limits and API errors gracefully
- Display loading states during AI processing

### Request Format
```typescript
// Standard request (no reasoning params)
{
  model: "openai/gpt-5-mini",
  messages: [...],
  stream: true
}
```

### UI Patterns for AI
- Use `AIThinkingIndicator` for loading states (simple spinner, no reasoning UI)
- Render markdown with `MarkdownRenderer` component
- Show suggestions as clickable buttons
- Implement copy, download, and retry actions
- Use `LiquidGlassModal` for AI chat interfaces
- **NO** deep thinking checkboxes
- **NO** reasoning toggle switches
- **NO** thinking step displays

### Error Handling
- Catch and display API errors in toast notifications
- Provide retry mechanisms for failed requests
- Never expose API keys or internal errors to users
- Log errors for debugging but show user-friendly messages
- Handle network timeouts gracefully (30s default)

### Context Management
- Smart AI: Gets all orders + page context
- Order AI: Gets single order + detailed ingredients
- Granulation: Gets formula ingredients array
- Recipe Generator: Gets target effects + constraints

### Model Selection Guidelines
- Use GPT-5 Mini for general Q&A (fast, cost-effective)
- Use GPT-5 for complex analysis requiring deep reasoning
- Use Claude for consensus/synthesis tasks
- Use Grok 4 for creative/alternative perspectives
- Run 3 models in parallel for comprehensive analysis

---

## Additional Resources

For more detailed guidance, see these documentation files:

### Exception Handling & Flexibility
üìñ **docs/FLEXIBILITY_GUIDE.md**
- When and how to bend the rules
- Design system extension procedures
- Performance-critical exemptions
- Real-world examples from this project

### Conflict Resolution
üìñ **docs/CONFLICT_RESOLUTION.md**
- Priority hierarchy explained
- 15+ common conflict scenarios with resolutions
- Decision-making framework
- Emergency procedures

### Monitoring & Continuous Improvement
üìñ **docs/MONITORING_GUIDE.md**
- Tracking build success, AI performance, design compliance
- Using existing tools (Vercel, git, grep)
- Quarterly review process
- Red flags and improvements

### Documentation Standards
üìñ **docs/DOCUMENTATION_STANDARDS.md**
- When to document vs when it's optional
- JSDoc templates
- Component documentation
- Lightweight ADR format
- Inline comment guidelines

---

**Last Updated**: 2025-10-21
**Version**: 3.0 (Major Enhancement: Added 5 new MCP servers, version control workflow, performance benchmarks, error logging)

### What's New in v3.0
- üÜï Added 5 new MCP servers (Filesystem, GitHub, Prisma, Playwright, Memory)
- üÜï Version control workflow (branch naming, PR process, code review)
- üÜï Performance benchmarks (specific metrics and targets)
- üÜï Error logging & monitoring guide
- ‚ú® Enhanced quick decision guide
- ‚ú® Cross-MCP workflow patterns
- ‚ú® MCP server selection matrix

